{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "O = \"开放性\"\n",
    "C = \"尽责性\"\n",
    "E = \"外向性\"\n",
    "A = \"宜人性\"\n",
    "N = \"神经质\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wenxinfeature 1039\n",
      "开放性\n",
      "item [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]\n",
      "weight [131.0, 127.0, 124.0, 102.0, 91.0, 82.0, 80.0, 79.0, 77.0, 75.0, 74.0, 70.0, 65.0, 64.0, 60.0, 58.0, 55.0, 37.0, 27.0, 23.0, 15.0, 8.0]\n",
      "['Pronoun', 'PPron', 'I', 'We', 'You', 'SheHe', 'They', 'iPron', 'Article', 'Verb', 'AuxVerb', 'enPast', 'enPresent', 'enFuture', 'Adverb', 'Preps', 'Conj', 'Negate', 'Quant', 'Number', 'Swear', 'YouPL']\n",
      "wenxin 22\n",
      "door 9.784915937985566\n",
      "o 1039\n",
      "尽责性\n",
      "item [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "weight [58.0, 55.0, 43.0, 26.0, 17.0, 9.0, 5.0, 2.0, 1.0, 1.0]\n",
      "['Pronoun', 'PPron', 'I', 'We', 'You', 'SheHe', 'They', 'iPron', 'Article', 'Verb']\n",
      "wenxin 10\n",
      "door 1.7583832275861397\n",
      "c 1039\n",
      "外向性\n",
      "item [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "weight [198.0, 94.0, 92.0, 89.0, 89.0, 72.0, 70.0, 54.0, 45.0, 40.0, 36.0, 35.0, 31.0, 29.0, 23.0, 18.0, 16.0, 14.0, 14.0]\n",
      "['Pronoun', 'PPron', 'I', 'We', 'You', 'SheHe', 'They', 'iPron', 'Article', 'Verb', 'AuxVerb', 'enPast', 'enPresent', 'enFuture', 'Adverb', 'Preps', 'Conj', 'Negate', 'Quant']\n",
      "wenxin 19\n",
      "door 7.1179388516823865\n",
      "e 1039\n",
      "宜人性\n",
      "item [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "weight [64.0, 49.0, 29.0, 29.0, 25.0, 23.0, 14.0, 13.0, 7.0, 5.0]\n",
      "['Pronoun', 'PPron', 'I', 'We', 'You', 'SheHe', 'They', 'iPron', 'Article', 'Verb']\n",
      "wenxin 10\n",
      "door 1.8707899673666983\n",
      "a 1039\n",
      "神经质\n",
      "item [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "weight [66.0, 66.0, 63.0, 51.0, 49.0, 45.0, 42.0, 38.0, 36.0, 33.0, 23.0, 16.0, 15.0, 11.0, 10.0, 10.0, 4.0]\n",
      "['Pronoun', 'PPron', 'I', 'We', 'You', 'SheHe', 'They', 'iPron', 'Article', 'Verb', 'AuxVerb', 'enPast', 'enPresent', 'enFuture', 'Adverb', 'Preps', 'Conj']\n",
      "wenxin 17\n",
      "door 3.7952047111414826\n",
      "n 1039\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd# 读取csv文件\n",
    "import numpy as np\n",
    "\n",
    "#显示所有列\n",
    "pd.set_option('display.max_columns', None)\n",
    "#显示所有行\n",
    "pd.set_option('display.max_rows', None)\n",
    "#设置value的显示长度为100，默认为50\n",
    "pd.set_option('max_colwidth',100)\n",
    "\n",
    "def get_feature(feature,f,origin,wenxin_co,result):\n",
    "    print(feature)\n",
    "    data = origin.copy(deep=True)# 深拷贝保留副本，此应为相关系数文件的副本\n",
    "    data = data[[\"数据\",feature,\"文心对应特征\"]]\n",
    "    wenxin = wenxin_co.copy(deep=True)#深拷贝保留副本,此为文心特征文件的拷贝\n",
    "    fg = f.copy(deep=True)# 此为feature文件的拷贝\n",
    "    data.sort_values(by=feature,axis=0,ascending=False,inplace=True)# ascending=False表示降序\n",
    "    num = 0\n",
    "    weight = []\n",
    "    item = []\n",
    "    for a in data[feature]:\n",
    "        num = num+1\n",
    "        if a > 0 :#<选负相关的\n",
    "            item.append(num)\n",
    "            weight.append(a*(1000))#扩大1000倍\n",
    "    print('item',item)\n",
    "    print('weight',weight)\n",
    "    \n",
    "    ind_value = []\n",
    "    for i in item:\n",
    "        ind_value.append(fg[\"英文\"][i])\n",
    "    '''\n",
    "    w = list(data[feature])\n",
    "    print('weight',w)\n",
    "    w = np.array(w[0:7])\n",
    "    weight = (w*f).tolist()\n",
    "    print('weight',weight)\n",
    "    '''\n",
    "    # 相关系数对应的文心特征项名，ind是相关系数.csv中的索引，要提取同文件中的文心特征索引\n",
    "    #ind_corre = [\"WordCount\",\"I\",\"You\",\"YouPL\",\"SheHe\",\"They\",\"NegEmo\",\"Anx\",\"Swear\",\"PosEmo\",\"NumEmotion\",\"Negate\",\"Funct\"]\n",
    "    #ind_value = []\n",
    "    '''\n",
    "    for i in ind:\n",
    "        #ind_value.append(ind_corre[i])\n",
    "        if type(ind_corre[i]) == list:\n",
    "            ind_value.extend(ind_corre[i])\n",
    "        else:\n",
    "            ind_value.append(ind_corre[i])\n",
    "         \n",
    "    '''\n",
    "    print(ind_value)\n",
    "    wenxin = wenxin[ind_value]\n",
    "    print('wenxin',len(ind_value))\n",
    "    wenxin = wenxin.rmul(weight) # 乘法\n",
    "    wenxin[\"Gather\"] = wenxin.sum(axis=1)\n",
    "    door = (wenxin[\"Gather\"].mean())\n",
    "    print('door',door)\n",
    "    wenxin = wenxin.assign(Lable = 0)#选负相关就为1\n",
    "    high = list((wenxin[wenxin[\"Gather\"]>=door]).index)\n",
    "    for i in high:  \n",
    "        wenxin.iloc[i,-1] = 1#选负相关就为0\n",
    "    name = '{value}'.format(value = feature)\n",
    "    result[name] = wenxin.iloc[:,-1]\n",
    "    return result\n",
    "\n",
    "data = pd.read_csv(r'D:\\weibo\\users.csv', usecols=[\"用户id\",\"粉丝数\",\"关注数\",\"简介\"], dtype={\"id\": str})# 只要这一列,读取为字符串类型\n",
    "datas = pd.DataFrame(columns = ['ID','FanNumber','ProfileLen','Focus'])\n",
    "datas['ID'] = data[\"用户id\"]\n",
    "datas[\"FanNumber\"] = data[\"粉丝数\"]\n",
    "datas['Focus'] = data[\"关注数\"]\n",
    "list_len = []\n",
    "for n in data[\"简介\"]:\n",
    "    if type(n) == str:\n",
    "        list_len.append(len(n))\n",
    "    else:\n",
    "        list_len.append(0)\n",
    "\n",
    "len_dic = {'ProfileLen':list_len}\n",
    "len_data = pd.DataFrame(len_dic)\n",
    "datas['ProfileLen'] = len_data['ProfileLen']\n",
    "\n",
    "file = pd.read_csv(\"corre.csv\",encoding = 'gb18030') # 编码方式：包含的字符个数：GB2312 < GBK < GB18030\n",
    "wenxin_fea = pd.read_csv(r\"D:\\wenxin_result\\文心特征1683598100582.csv\")\n",
    "\n",
    "wenxin_feature = pd.concat([wenxin_fea,datas],axis = 1)\n",
    "print('wenxinfeature',len(wenxin_feature))\n",
    "graf = pd.read_csv(r\"D:\\wenxin\\WenXinV4.0\\feature.csv\", usecols=[\"索引\",\"英文\"])\n",
    "for i in range(24):\n",
    "    if file[\"文心对应特征\"][i] =='FALSE':\n",
    "        file.drop(i,axis=0,inplace=False) \n",
    "    else:\n",
    "        continue\n",
    "res = pd.DataFrame(wenxin_feature[\"ID\"])\n",
    "#print(res)\n",
    "res = get_feature(O,graf,file,wenxin_feature,res)\n",
    "print('o',len(res))\n",
    "res = get_feature(C,graf,file,wenxin_feature,res)\n",
    "print('c',len(res))\n",
    "res = get_feature(E,graf,file,wenxin_feature,res)\n",
    "print('e',len(res))\n",
    "res = get_feature(A,graf,file,wenxin_feature,res)\n",
    "print('a',len(res))\n",
    "res = get_feature(N,graf,file,wenxin_feature,res)\n",
    "print('n',len(res))\n",
    "\n",
    "res.to_csv(\"./sentiment/personality_label.csv\",encoding='utf_8_sig', mode='w', index=False)\n",
    "print(\"DONE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
