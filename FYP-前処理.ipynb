{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze and process text data in CSV files, with the following ideas:\n",
    "1. Using the Pandas to read in CSV files as dataframes\n",
    "2. Convert dataframe to Python object\n",
    "3. Write a processing function (such as data manipulation function and word segmentation)\n",
    "4. Call the processing function here to obtain new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from harvesttext import HarvestText\n",
    "import pyhanlp\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "def csv_read_by_id(uid):\n",
    "    path = \"D://weibo//\" + uid + \"//\" + uid + \".csv\"#打开该用户文件夹并提取同名csv文件\n",
    "    df = pd.read_csv(path, usecols=[\"正文\"], dtype={\"id\": str})# 只要这一列,读取为字符串\n",
    "    #print(df)\n",
    "    contend = []\n",
    "    for i in df[\"正文\"]:\n",
    "        i = clean_txt(i)\n",
    "        contend.append(str(i)+'\\n')\n",
    "    #print('contend',contend)\n",
    "    return contend\n",
    "\n",
    "def is_chinese(uchar):\n",
    "    if uchar >= u'\\u4e00' and uchar <= u'\\u9fa5':  # 判断一个uchar是否是汉字\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def only_chinese(contents):\n",
    "    content = ''\n",
    "    for i in contents:\n",
    "        if is_chinese(i):\n",
    "            content = content+i\n",
    "    return content\n",
    "\n",
    "def clean_txt(s):\n",
    "    # 清洗数据，只保留汉字\n",
    "    ht = HarvestText()\n",
    "    CharTable = pyhanlp.JClass('com.hankcs.hanlp.dictionary.other.CharTable')\n",
    "    content = CharTable.convert(str(s))\n",
    "    cleaned_content = ht.clean_text(content, emoji=True,weibo_topic=True)  # 去除链接、@xxx、email、话题、表情\n",
    "    final = only_chinese(cleaned_content)\n",
    "    return final\n",
    "\n",
    "def write_list_in_txt(fn,content):\n",
    "    file_handle = open(fn, 'w', encoding='utf-8-sig')\n",
    "    file_handle.writelines(content)\n",
    "    file_handle.close()\n",
    "    \n",
    "def write_byte_in_txt(fn,content):\n",
    "    file_handle = open(fn,\"wb\")\n",
    "    file_handle.write(content.encode('utf-8'))\n",
    "    file_handle.close()  \n",
    "               \n",
    "def cut_stopwords(tokens):\n",
    "    stop_word = ''\n",
    "    stop_words = open(r'D:\\corpus\\stopwords.txt',\"rb\").read()\n",
    "    for i in stop_words:\n",
    "        if i=='\\n':\n",
    "            stop_word = stop_word+i\n",
    "    tokens = [t for token in tokens for t in token if t != '' and t not in stop_word]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import jieba # 使用jieba分词\n",
    "def reduce_nan(uid):\n",
    "    '''删除csv正文中的空字符串'''\n",
    "    file_name = \"D://weibo//\" + uid + \"//\" + uid + \".csv\"#打开该用户文件夹并提取同名csv文件\n",
    "    #file = pd.read_csv(file_name, usecols=[\"正文\"], dtype={\"id\": str})# 只要这一列,读取为字符串\n",
    "    list_line = []\n",
    "    a = 0\n",
    "    with open(file_name,'r',encoding = 'utf8') as file:\n",
    "        for i in file.readlines():\n",
    "                s = i.split(',')\n",
    "                if s[2] != '正文':\n",
    "                    list_line.append(s[2].rstrip()+'\\n')\n",
    "    return list_line\n",
    "\n",
    "def count_weibo(file_name):\n",
    "    count = 0\n",
    "    with open(file_name,'r',encoding = 'utf8') as file:\n",
    "        for i in file.readlines():\n",
    "                count = count+1\n",
    "    return count\n",
    "\n",
    "# 预处理：读取，分词\n",
    "#path = r'D:\\weibo\\users.csv'\n",
    "#data = pd.read_csv(path, usecols=[\"用户id\"], dtype={\"id\": str})# 只要这一列,读取为字符串类型\n",
    "\n",
    "#counting = 0\n",
    "for n in data[\"用户id\"]:\n",
    "\n",
    "    i = str(n)\n",
    "    \n",
    "    filename = \"D://corpus//\" + i + \".txt\"\n",
    "    output = \"D://result//\" + i + \".txt\"\n",
    "    txt_list = csv_read_by_id(i)\n",
    "\n",
    "    #数据写入txt，一条微博为一行\n",
    "    \n",
    "    with open(filename,\"rb\") as f:\n",
    "        for j in f.read():\n",
    "            #print(j)\n",
    "            #print(type(j))\n",
    "           \n",
    "            if j == 239:\n",
    "                try:\n",
    "                    txt_list = reduce_nan(i)\n",
    "                    write_list_in_txt(filename,txt_list)  \n",
    "                except:\n",
    "                    print(i)\n",
    "                    break  \n",
    "            \n",
    "    \n",
    "    #counting += count_weibo(filename)\n",
    "    \n",
    "    \n",
    "    # 读取到该用户的正文开始分词(这里没有启用并行模式的原因是windows系统不支持)\n",
    "    word = \" \".join(jieba.cut(content))# 精确模式（默认），分隔符为空格\n",
    "    #去停用词(结合了中文停用词表、哈工大停用词表、四川大学机器智能实验室停用词库)\n",
    "    #拿到了分词后的文件，在一般的NLP处理中，会需要去停用词。由于word2vec的算法依赖于上下文，而上下文有可能就是停词。因此对于word2vec，我们可以不用去停词。\n",
    "    #words = \" \".join(cut_stopwords(word))\n",
    "    write_byte_in_txt(output,word)\n",
    "    #training(output)\n",
    "    \n",
    "#print(counting)\n",
    "print('OK')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
